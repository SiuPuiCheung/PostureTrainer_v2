# Deep Research Prompt – Automated Key-Pose Extraction & Labeling for Custom Movement Routines

## 1. Focus
How can a desktop-first posture & movement training MVP automatically extract, rank, and label key poses (and transitional segments) from raw movement capture to accelerate authoring of multi-step routines (kata, dance phrases, rehab flows) under tight resource constraints (<500 AUD, local processing, minimal dependencies)?

## 2. Research Objective
Identify lightweight, explainable, and robust methods to: (a) segment continuous motion into candidate steps/key-poses, (b) assign human-meaningful labels or descriptors, (c) allow quick human refinement, and (d) produce a versioned routine template (JSON) with tolerances. Emphasis on modularity, incremental adoption (start simple heuristic → optional advanced ML), and resilience to tempo variance, body shape differences, and moderate occlusions.

## 3. Core Research Questions
1. Which algorithms (velocity extrema, acceleration/jerk thresholds, change-point detection, clustering, self-similarity matrices, graph partitioning) yield the best cost–quality trade-off for key-pose extraction?
2. How do academic systems for dance, martial arts, rehab define and validate “key poses”? (Pose salience definitions, biomechanical events, semantic anchors.)
3. What minimal feature set (joint angles, landmark positions, derived symmetry/balance metrics) suffices for robust segmentation without expensive 3D inference?
4. How can we combine automatic extraction with human-in-loop adjustments efficiently (UI affordances, suggested merges/splits, ranking of uncertain segments)?
5. Which clustering strategies (KMeans, agglomerative, HDBSCAN, spectral) handle variable-length sequences and variability in user execution while remaining interpretable?
6. How to detect transitions vs stable poses (pose stability windows, variance thresholds, dwell-time)?
7. How can LLMs assist labeling (convert brief user description + extracted pose stats into semantic step labels) while staying local/offline where possible?
8. What evaluation metrics (Pose Stability Score, Segmentation Precision/Recall vs hand labels, Mean Temporal Offset, Label Coherence) are practical for iterative improvement?
9. How to gracefully degrade on low FPS / partial occlusion (pose confidence weighting, smoothing filters, fallback heuristics)?
10. How to integrate tolerance estimation (angle variance envelopes, timing flex windows) directly from extraction statistics?
11. What data structures (annotated timeline, pose graph, step array with metadata) maximize downstream alignment efficiency (DTW / rule-based comparison)?
12. How to future-proof for style variants (different correct key-poses for same semantic step) and optional personalization (adaptive templates)?

## 4. Comparative Landscape (Representative Domains)
- Martial arts kata segmentation (temporal landmark stability & technique checkpoints)
- Dance phrase boundary detection (self-similarity matrices, novelty scores)
- Rehabilitation motion analysis (rom event detection: max flexion/extension)
- Fitness rep segmentation (peak velocity & reversal point detection)
- Gesture recognition toolkits (state machine transitions, HMM hidden states)

## 5. Technical Dimensions to Map
| Dimension | Candidates | Evaluation Focus |
|-----------|-----------|------------------|
| Signal Preprocessing | Smoothing (Savitzky-Golay), EMA, median | Noise reduction vs latency |
| Salience Features | Joint angle velocity/accel/jerk, centroid shift, symmetry deltas | Discriminative power & cost |
| Change-Point Detection | Window variance, PELT, binary segmentation, Bayesian online | Accuracy vs simplicity |
| Clustering for Stable Poses | KMeans, HDBSCAN (noise tolerant), hierarchical | Interpretability, parameter sensitivity |
| Transition Detection | Variance spikes, velocity threshold crossing, dynamic thresholds | False split/merge rate |
| Label Generation | User seed labels, LLM assisted, ontology mapping | Human effort reduction |
| Tolerance Derivation | Empirical angle SD, IQR, dynamic scaling by difficulty | Fairness & personalization |
| Data Model | Flat step list JSON, graph with alternate variants, version tags | Extensibility |
| Confidence Handling | Landmark visibility weighting, per-joint reliability | Robustness to occlusion |
| Incremental Operation | Streaming buffer, online segmentation vs batch | Latency & resource use |

## 6. Recommended MVP Extraction Pipeline (Incremental)
1. Capture landmark stream (MediaPipe) → store (x,y,visibility) per frame.
2. Compute derived features per frame: joint angles (selected: elbows, shoulders, hips, knees), angle velocity & sign changes, centroid, symmetry (L/R pairs), posture alignment.
3. Smoothing: apply short window Savitzky-Golay (e.g., window=7) for angles; clamp extremes.
4. Pose Stability Detection: mark frames where all monitored angle velocities |v| < threshold T_v (adaptive: percentile-based) for dwell >= min_dwell_frames.
5. Candidate Key-Pose Consolidation: merge adjacent stability windows separated by tiny gaps (< gap_merge_frames). Select representative frame (e.g., lowest aggregate velocity or median of window).
6. Transition Boundaries: start/end of stability windows define step segmentation; transitional frames in between considered motion segments.
7. Ranking & Filtering: score key-poses by (dwell_time * average_visibility * inverse mean velocity). Drop very short/low-confidence candidates.
8. Automatic Label Suggestion: compute salient differences from previous key-pose (e.g., angle delta > threshold); produce descriptor tokens ("left_arm_raise", "torso_rotate"). Optionally pass tokens + minimal context to local/remote LLM for polished label.
9. Tolerance Estimation: For each pose window, compute angle SD per joint; set angle tolerance = clamp(k * SD, min_angle_tol, max_angle_tol). Timing tolerance proportional to dwell_time * factor.
10. Export JSON template with steps = [{id, label, keypose(angles subset), duration_estimate, tolerance, metadata}].
11. Human-in-loop UI: present timeline, allow merges, re-label, adjust tolerances, reorder.

## 7. Extended JSON Schema (Pose Extraction Metadata)
```json
{
  "routine_name": "Example Routine",
  "extraction": {
    "method": "velocity_stability_v1",
    "sampling_fps": 30,
    "preprocessing": {"smoothing": "savitzky_golay", "window": 7},
    "feature_joints": ["left_elbow","right_elbow","left_knee","right_knee","left_shoulder","right_shoulder","left_hip","right_hip"],
    "thresholds": {"velocity": 0.025, "min_dwell_frames": 9}
  },
  "steps": [
    {
      "id": 1,
      "label": "Guard Stance",
      "keypose": {"angles": {"left_elbow": 160, "right_elbow": 160, "left_knee": 175, "right_knee": 175}},
      "duration_ms": 1200,
      "tolerance": {"angle_deg": {"default": 15}, "timing_ms": 400},
      "confidence": 0.92,
      "source_window": {"start_frame": 45, "end_frame": 82},
      "features": {"symmetry_index": 0.05}
    }
  ],
  "metadata": {"created": "2025-11-29", "version": 1}
}
```

## 8. Pseudocode – Minimal Extraction
```python
def extract_keyposes(frames, angle_fn, velocity_threshold, min_dwell):
    angles = [angle_fn(f) for f in frames]  # returns dict joint->deg
    # compute per-frame velocity magnitude over selected joints
    vel = []
    for i in range(len(angles)):
        if i == 0:
            vel.append(0.0)
        else:
            diff = [abs(angles[i][j] - angles[i-1][j]) for j in angles[i]]
            vel.append(sum(diff) / len(diff))
    stable = []
    start = None
    for i, v in enumerate(vel):
        if v < velocity_threshold:
            if start is None: start = i
        else:
            if start is not None and (i - start) >= min_dwell:
                stable.append((start, i-1))
            start = None
    if start is not None and (len(vel) - start) >= min_dwell:
        stable.append((start, len(vel)-1))
    keyposes = []
    for (a,b) in stable:
        # representative frame: lowest velocity frame
        rep = min(range(a,b+1), key=lambda k: vel[k])
        keyposes.append({"frame": rep, "window": (a,b), "angles": angles[rep]})
    return keyposes
```

## 9. Enhanced Features (Phase 2)
- Adaptive velocity thresholds: percentile-based per sequence.
- Change-point detection overlay: confirm boundaries with PELT (cost = angle variance).
- Cluster representative poses (KMeans on angle vectors of stable windows) to consolidate duplicates.
- Confidence weighting: ignore joints with visibility < vis_min.
- Multi-variant support: store alternate poses for same semantic label.

## 10. Label Generation Strategies
- Heuristic tokens: derive verbs/nouns from angle deltas ("raise_arm", "rotate_torso").
- Template-based LLM: prompt with tokens + context ("Provide a concise coaching-friendly label.").
- Ontology mapping: maintain mapping table for standard domain terms (martial_arts → "guard", dance → "arabesque").
- Human confirmation loop: highlight low-confidence segments for manual naming.

## 11. Evaluation Metrics
- Segmentation Precision/Recall against hand-labeled ground truth.
- Mean Temporal Offset of boundaries (ms or frames).
- Key-Pose Angle Stability Score (mean intra-window angle SD).
- Label Acceptance Rate (human approved without edit).
- Authoring Time Reduction (% vs manual from scratch).
- Runtime Latency per frame (ms on reference hardware).

## 12. Experiments & Test Plan
1. Collect small labeled dataset (5 kata, 5 dance phrases, 5 rehab exercises). Hand-annotate key-pose frames.
2. Compare heuristics vs clustering vs change-point for segmentation quality.
3. Measure latency and memory footprint for each method.
4. A/B labeling assistance: heuristic-only vs heuristic+LLM; measure human edit rate.
5. Stress test on noisy capture: simulate reduced visibility, lower FPS.
6. Tolerance derivation: evaluate fairness (false negative rate) using extracted vs fixed tolerances.

## 13. Risks & Mitigations
- Over-segmentation: apply minimum dwell & merge pass; cluster duplicates.
- Under-segmentation: dynamic threshold lowering + highlight high residual velocity regions.
- Label ambiguity: show angle deltas in UI; provide synonyms via ontology.
- Style variance misclassification: support variants array per step; flexible tolerance scaling.
- Occlusion-induced false boundaries: incorporate visibility weighting & smoothing.
- User confusion: progressive disclosure—show simple summary first, details on demand.

## 14. Deliverables
- Segmentation module (Python) with heuristic baseline.
- Evaluation notebook comparing methods.
- Extended JSON schema for extracted routine templates.
- Label assistance prompt templates.
- Metrics dashboard (authoring time, precision/recall).

## 15. Success Criteria
- ≥85% segmentation F1 on mixed-domain dataset (baseline heuristics + minimal tuning).
- Average authoring time reduction ≥50% vs manual.
- Label auto-accept rate ≥60% for first pass.
- Runtime overhead ≤10ms/frame on mid-range laptop.
- False negative (missed key-pose) rate ≤10%.

## 16. MVP Recommendation Summary
- Start with velocity + dwell heuristic (fast, explainable), enrich with clustering only if duplicates proliferate.
- Use adaptive thresholds (percentile) to generalize across tempos.
- Provide transparent UI for merges/splits; always show confidence & stability metrics.
- Extract tolerance directly from intra-window angle variance for personalization.
- Integrate lightweight LLM labeling optionally (fallback to heuristic tokens offline).

---
Use this prompt as a roadmap for implementing automated key-pose extraction and labeling aligned with low-cost, explainable, and extensible MVP goals.
